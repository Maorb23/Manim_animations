{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic regression with PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim --disable_caching -qm -v WARNING SigmoidSurfacesLast_one\n",
    "\n",
    "class SigmoidSurfacesLast_one(ThreeDScene):\n",
    "    def construct(self):\n",
    "\n",
    "\n",
    "        X_new = X[::15]\n",
    "        y_new = y[::15]\n",
    "        # Define the ranges based on your data\n",
    "        x0_min, x0_max = X_new[:, 0].min(), X_new[:, 0].max()\n",
    "        x1_min, x1_max = X_new[:, 1].min(), X_new[:, 1].max()\n",
    "        \n",
    "        # Create the 3D axes\n",
    "        axes = ThreeDAxes(\n",
    "            x_range=[x0_min, x0_max],\n",
    "            y_range=[x1_min, x1_max],\n",
    "            z_range=[-0.2, 1.2],\n",
    "            x_length=7,\n",
    "            y_length=7,\n",
    "            z_length=5,\n",
    "        )\n",
    "        self.add(axes)\n",
    "        \n",
    "        # Define the sigmoid function\n",
    "        def sigmoid(x, y, alpha, beta0, beta1):\n",
    "            return 1 / (1 + np.exp(-(alpha + beta0 * x + beta1 * y)))\n",
    "        \n",
    "        num_samples = 20  # Adjust as needed\n",
    "        indices = np.random.choice(len(alpha_samples), size=num_samples, replace=False)\n",
    "\n",
    "        for idx in indices:\n",
    "            a = alpha_samples[idx]\n",
    "            b0 = beta_samples0[idx]\n",
    "            b1 = beta_samples1[idx]\n",
    "\n",
    "            surface = Surface(\n",
    "                lambda u, v: axes.c2p(\n",
    "                    u,\n",
    "                    v,\n",
    "                    sigmoid(u, v, a, b0, b1)\n",
    "                ),\n",
    "                u_range=[x0_min, x0_max],\n",
    "                v_range=[x1_min, x1_max],\n",
    "                resolution=(15, 15),\n",
    "                fill_opacity=0.2,\n",
    "                stroke_color=BLACK,\n",
    "                checkerboard_colors= False,\n",
    "                \n",
    "            )\n",
    "            self.add(surface)\n",
    "            surface.set_color(GRAY)\n",
    "        # Compute the mean parameters\n",
    "        mean_alpha = np.mean(alpha_samples)\n",
    "        mean_beta0 = np.mean(beta_samples0)\n",
    "        mean_beta1 = np.mean(beta_samples1)\n",
    "\n",
    "        # Create the mean sigmoid surface\n",
    "        mean_surface = Surface(\n",
    "            lambda u, v: axes.c2p(\n",
    "                u,\n",
    "                v,\n",
    "                sigmoid(u, v, mean_alpha, mean_beta0, mean_beta1)\n",
    "            ),\n",
    "            u_range=[x0_min, x0_max],\n",
    "            v_range=[x1_min, x1_max],\n",
    "            resolution=(25, 25),\n",
    "            fill_opacity=0.9,\n",
    "            checkerboard_colors= False,\n",
    "        )\n",
    "        self.add(mean_surface)\n",
    "        mean_surface.set_color(MAROON) \n",
    "        \n",
    "        # Add data points to the scene\n",
    "        data_points = VGroup()\n",
    "        for i in range(len(X_new)):\n",
    "            dot = Dot3D(\n",
    "                point=axes.c2p(X_new[i, 0], X_new[i, 1], y_new[i]),\n",
    "                radius=0.05,\n",
    "                color=BLACK,\n",
    "            )\n",
    "            data_points.add(dot)\n",
    "        self.add(data_points)\n",
    "        \n",
    "        # Set initial camera orientation\n",
    "        self.set_camera_orientation(phi=45 * DEGREES, theta=-45 * DEGREES)\n",
    "\n",
    "        \n",
    "        # Start ambient camera rotation\n",
    "        self.begin_ambient_camera_rotation(rate=0.5)\n",
    "        self.wait(17)  # Adjust the duration as needed\n",
    "        \n",
    "        # Stop the camera rotation\n",
    "        self.stop_ambient_camera_rotation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear regression with pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim --disable_caching -ql -v WARNING LinearSurfaces40\n",
    "\n",
    "class LinearSurfaces40(ThreeDScene):\n",
    "    def construct(self):\n",
    "\n",
    "\n",
    "        # Subsample data for plotting\n",
    "        X_new = X[::5]\n",
    "        y_new = y[::5]\n",
    "\n",
    "        # Define the ranges based on your data\n",
    "        x0_min, x0_max = X_new[:, 0].min(), X_new[:, 0].max()\n",
    "        x1_min, x1_max = X_new[:, 1].min(), X_new[:, 1].max()\n",
    "        \n",
    "        # Compute the mean parameters\n",
    "        mean_alpha = np.mean(intercept)\n",
    "        mean_beta0 = np.mean(slopes0)\n",
    "        mean_beta1 = np.mean(slopes1)\n",
    "\n",
    "        # Define the Linear model function\n",
    "        def linear_model(x, y, alpha, beta0, beta1):\n",
    "            return alpha + beta0 * x + beta1 * y\n",
    "\n",
    "        # Compute z-values at the corners of the x-y plane using mean parameters\n",
    "        z_corners = [\n",
    "            linear_model(x0_min, x1_min, mean_alpha, mean_beta0, mean_beta1),\n",
    "            linear_model(x0_min, x1_max, mean_alpha, mean_beta0, mean_beta1),\n",
    "            linear_model(x0_max, x1_min, mean_alpha, mean_beta0, mean_beta1),\n",
    "            linear_model(x0_max, x1_max, mean_alpha, mean_beta0, mean_beta1),\n",
    "        ]\n",
    "\n",
    "        # Compute z_min and z_max considering both data and model outputs\n",
    "        z_min = min(y_new.min(), min(z_corners)) - 3  # Subtract buffer if needed\n",
    "        z_max = max(y_new.max(), max(z_corners)) + 3  # Add buffer if needed\n",
    "\n",
    "        # Create the 3D axes with the correct z_range and include numbers\n",
    "        axes = ThreeDAxes(\n",
    "            x_range=[x0_min - 3, x0_max + 3, (x0_max + 3 - (x0_min - 3)) / 5],\n",
    "            y_range=[x1_min - 3, x1_max + 3, (x1_max + 3 - (x1_min - 3)) / 5],\n",
    "            z_range=[z_min, z_max, (z_max - z_min) / 5],\n",
    "            x_length=7,\n",
    "            y_length=7,\n",
    "            z_length=5,\n",
    "        )\n",
    "        self.add(axes)\n",
    "\n",
    "        # Add numbers to the axes\n",
    "        x_numbers = VGroup()\n",
    "        y_numbers = VGroup()\n",
    "        z_numbers = VGroup()\n",
    "\n",
    "        # Get the ticks positions\n",
    "        z_ticks = np.arange(0,8,1).astype(int)\n",
    "\n",
    "        # Create labels for z-axis\n",
    "        for z in z_ticks:\n",
    "            z_label = Text(f\"{z:.1f}\", font_size=24)\n",
    "            z_label.move_to(axes.c2p(0, 0, z))\n",
    "            z_label.rotate(PI / 2, axis=UP)\n",
    "            z_label.shift(LEFT * 0.1)\n",
    "            #z_label.always_face_camera(self.camera)\n",
    "            z_numbers.add(z_label)\n",
    "\n",
    "        # Add labels to scene\n",
    "        self.add(axes, x_numbers, y_numbers, z_numbers)\n",
    "\n",
    "        num_samples = 36  # Total number of samples\n",
    "        group_size = 9    # Size of each group\n",
    "\n",
    "        # We'll use the first num_samples indices\n",
    "        indices = np.arange(0, 7, 1).astype(int)\n",
    "\n",
    "        # Split indices into groups of group_size\n",
    "        groups = [indices[i:i + group_size] for i in range(0, num_samples, group_size)]\n",
    "\n",
    "        # Plot the mean regression plane for each group\n",
    "        for idx, group in enumerate(groups):\n",
    "            a_mean = np.mean(intercept[group])\n",
    "            b0_mean = np.mean(slopes0[group])\n",
    "            b1_mean = np.mean(slopes1[group])\n",
    "\n",
    "            surface = Surface(\n",
    "                lambda u, v: axes.c2p(\n",
    "                    u,\n",
    "                    v,\n",
    "                    linear_model(u, v, a_mean, b0_mean, b1_mean)\n",
    "                ),\n",
    "                u_range=[x0_min, x0_max],\n",
    "                v_range=[x1_min, x1_max],\n",
    "                resolution=(15, 15),\n",
    "                fill_opacity=0.2,\n",
    "                color=BLUE,\n",
    "            )\n",
    "            self.add(surface)\n",
    "\n",
    "        # Create the overall mean linear model surface\n",
    "        mean_surface = Surface(\n",
    "            lambda u, v: axes.c2p(\n",
    "                u,\n",
    "                v,\n",
    "                linear_model(u, v, mean_alpha, mean_beta0, mean_beta1)\n",
    "            ),\n",
    "            u_range=[x0_min, x0_max],\n",
    "            v_range=[x1_min, x1_max],\n",
    "            resolution=(25, 25),\n",
    "            fill_opacity=0.7,\n",
    "            stroke_opacity=0.5,\n",
    "            color=MAROON,\n",
    "        )\n",
    "        self.add(mean_surface)\n",
    "\n",
    "        # Add data points to the scene\n",
    "        data_points = VGroup()\n",
    "        for i in range(len(X_new)):\n",
    "            dot = Dot3D(\n",
    "                point=axes.c2p(X_new[i, 0], X_new[i, 1], y_new[i]),\n",
    "                radius=0.05,\n",
    "                color=BLACK,\n",
    "            )\n",
    "            data_points.add(dot)\n",
    "        self.add(data_points)\n",
    "\n",
    "        # Set initial camera orientation\n",
    "        self.set_camera_orientation(phi=45 * DEGREES, theta=-45 * DEGREES, zoom=1.2)\n",
    "\n",
    "        # Start ambient camera rotation\n",
    "        self.begin_ambient_camera_rotation(rate=0.5)\n",
    "        self.wait(1)  # Adjust the duration as needed\n",
    "\n",
    "        # Stop the camera rotation\n",
    "        self.stop_ambient_camera_rotation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -qm -v WARNING NeuralNetworkSceneLastMB\n",
    "\n",
    "\n",
    "class NeuralNetworkSceneLastMB(Scene):\n",
    "\n",
    "    def construct(self):\n",
    "        # Create layers without layer labels\n",
    "        input_layer = self.create_layer(3, \"x\")\n",
    "        hidden_layer = self.create_layer(4, \"h\")\n",
    "        output_layer = self.create_layer(2, \"z\")\n",
    "        Intro_text = Text(\"MB\", font_size=24, weight= BOLD,color= BLUE_D).to_edge(UP).shift(LEFT*6.05)\n",
    "        self.add(Intro_text)\n",
    "        Neural_net_text = Text(\"Neural Network\", font_size=16, weight= BOLD,color= BLUE_E).next_to(Intro_text,DOWN,buff=0.2) #.shift(RIGHT*0.1)\n",
    "        self.add(Neural_net_text)\n",
    "\n",
    "        # Position layers\n",
    "        layers = VGroup(input_layer, hidden_layer, output_layer)\n",
    "        layers.arrange(RIGHT, buff=2.7)\n",
    "        self.add(layers)\n",
    "        self.layers = layers\n",
    "\n",
    "        # Create layer labels\n",
    "        input_label = Text(\"Input Layer\", font_size=24)\n",
    "        hidden_label = Text(\"Hidden Layer\", font_size=24)\n",
    "        output_label = Text(\"Output Layer\", font_size=24)\n",
    "        layer_labels = VGroup(input_label, hidden_label, output_label)\n",
    "\n",
    "        # Position layer labels above the layers at the same height\n",
    "        for label, layer in zip(layer_labels, layers):\n",
    "            label.move_to(layer.get_center())\n",
    "            label.shift(UP * 3.2)  \n",
    "\n",
    "        self.add(layer_labels)\n",
    "        self.layer_labels = layer_labels\n",
    "\n",
    "        # Draw connections with weights\n",
    "        connections_ih, weights_ih,weights_ih_after = self.connect_layers(input_layer, hidden_layer, \"w\")\n",
    "        connections_oh, weights_ho,weights_oh_after = self.connect_layers(hidden_layer, output_layer, \"v\")\n",
    "        self.connections_oh = connections_oh\n",
    "        self.connections_ih = connections_ih\n",
    "\n",
    "        # Combine all connections and weights\n",
    "        connections = connections_ih + connections_oh\n",
    "        weights = weights_ih + weights_ho\n",
    "        weights_after = weights_ih_after + weights_oh_after\n",
    "        self.weights_after = weights_after\n",
    "\n",
    "        # Add connections and weight labels to the scene\n",
    "        self.add(connections, weights)\n",
    "\n",
    "        # Animate forward pass with computations\n",
    "        self.forward_pass(input_layer, hidden_layer, output_layer, connections, weights)\n",
    "\n",
    "        # Animate backpropagation with derivatives\n",
    "        self.backpropagation(input_layer, hidden_layer, output_layer, connections, weights)\n",
    "\n",
    "    def animate_wave(self, connections, color, direction='forward', run_time=1):\n",
    " \n",
    "        dots = VGroup()\n",
    "        #lines = VGroup() #optional\n",
    "        animations = []\n",
    "        \n",
    "        for connection in connections:\n",
    "            # Reverse the connection path if direction is 'backward'\n",
    "            if direction == 'backward':\n",
    "                path = Line(\n",
    "                    connection.get_end(),\n",
    "                    connection.get_start(),\n",
    "                    stroke_color=connection.get_stroke_color(),\n",
    "                    stroke_width=connection.get_stroke_width()\n",
    "                )\n",
    "            else:\n",
    "                path = connection.copy()\n",
    "            \n",
    "            # Create a dot at the start of the path\n",
    "            dot = Dot(color=color, radius=0.05)\n",
    "            dot.move_to(path.get_start())\n",
    "            dots.add(dot)\n",
    "            \n",
    "            # Animate the dot moving along the path\n",
    "            animation = MoveAlongPath(dot, path, run_time=run_time, rate_func=linear)\n",
    "            animations.append(animation)\n",
    "        \n",
    "        \n",
    "        #self.add(lines) #optional\n",
    "        #self.remove(lines) #optional\n",
    "        self.add(dots)\n",
    "        self.play(*animations, lag_ratio=0)\n",
    "        self.remove(dots)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def create_layer(self, num_neurons, label_prefix):\n",
    "        \"\"\"\n",
    "        Creates a layer with a specified number of neurons and labels each neuron uniquely.\n",
    "        \"\"\"\n",
    "        neurons = VGroup()\n",
    "        neuron_labels = VGroup()\n",
    "        for i in range(num_neurons):\n",
    "            neuron = Circle(radius=0.3, color=BLUE, fill_opacity=0)\n",
    "            neurons.add(neuron)\n",
    "        neurons.arrange(DOWN, buff=1)  # Increased buff for more vertical spacing\n",
    "        for i, neuron in enumerate(neurons):\n",
    "            # Create unique labels like x₁, x₂, etc.\n",
    "            label = MathTex(f\"{label_prefix}_{{{i+1}}}\")\n",
    "            label.scale(0.6)\n",
    "            label.next_to(neuron, UP, buff=0.05)  # if we want to change the label to be inside the circle, change LEFT to UP\n",
    "            neuron_labels.add(label)\n",
    "        layer = VGroup(neurons, neuron_labels)\n",
    "        return layer  # Return layer without layer label\n",
    "\n",
    "    def connect_layers(self, layer1, layer2, weight_prefix):\n",
    "\n",
    "        connections = VGroup()\n",
    "        weight_labels = VGroup()\n",
    "        weight_labels_after = VGroup()\n",
    "        neurons1 = layer1[0]  \n",
    "        neurons2 = layer2[0]\n",
    "        for i, neuron1 in enumerate(neurons1):\n",
    "            for j, neuron2 in enumerate(neurons2):\n",
    "                connection = Line(\n",
    "                    neuron1.get_right(),\n",
    "                    neuron2.get_left(),\n",
    "                    stroke_color=GREY,\n",
    "                    stroke_width=2\n",
    "                )\n",
    "                connections.add(connection)\n",
    "                weight_label = MathTex(f\"{weight_prefix}_{{{j+1}{i+1}}}\")\n",
    "                weight_label_new = MathTex(f\"\\\\tilde {weight_prefix}_{{{j+1}{i+1}}}\",font_size=16)\n",
    "                weight_label.scale(0.5)\n",
    "                midpoint = connection.get_midpoint()\n",
    "                direction = neuron2.get_left() - neuron1.get_right()\n",
    "                unit_direction = direction / np.linalg.norm(direction)\n",
    "                perp_direction = np.array([-unit_direction[1], unit_direction[0], 0])\n",
    "                weight_label.move_to(midpoint + perp_direction * 0.5)\n",
    "                weight_labels.add(weight_label)\n",
    "                weight_label_new.move_to(midpoint + perp_direction * 0.5)\n",
    "                weight_labels_after.add(weight_label_new)\n",
    "\n",
    "        return connections, weight_labels,weight_labels_after\n",
    "\n",
    "    def forward_pass(self, input_layer, hidden_layer, output_layer, connections, weights):\n",
    "\n",
    "        forward_pass = Text(\"Forward Pass\", font_size=30)\n",
    "        forward_pass.to_corner(DOWN + LEFT)\n",
    "        SIGMOID = MathTex(\"\\\\sigma(x) = \\\\frac{1}{1 + e^{-x}}\",font_size=20)\n",
    "        Bias = MathTex(\"b_j, c_t, \\\\quad j \\\\in [1,4], t \\\\in [1,2]: \\quad \\\\text{Bias  Terms}\",font_size=20)\n",
    "        SIGMOID.to_corner(UP + RIGHT).shift(DOWN * 0.6)  \n",
    "        Bias.next_to(SIGMOID,DOWN).shift(DOWN * 0.05,LEFT * 0.7)\n",
    "        self.add(forward_pass)\n",
    "        self.forward_pass_text = forward_pass\n",
    "        self.add(SIGMOID)\n",
    "        self.add(Bias)\n",
    "        self.SIGMOID = SIGMOID\n",
    "        self.Bias = Bias\n",
    "        self.wait(2)\n",
    "\n",
    "        \n",
    "        self.play(FadeOut(connections), FadeOut(weights)) # Hide connections and weights during computations\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in input_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        connections_ih_to_hidden = self.connections_ih[:len(input_layer[1]) * len(hidden_layer[1])]\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[conn.animate.set_color(YELLOW) for conn in connections_ih_to_hidden],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show computations at hidden layer\n",
    "        computations = VGroup()\n",
    "        for i, neuron_label in enumerate(hidden_layer[1]):\n",
    "            # Display computation next to each hidden neuron\n",
    "            comp = MathTex(f\"=\\\\sigma\\\\left(\\\\sum_{{j=1}}^{{3}} w_{{{i+1}j}} x_j + b_{{{i+1}}}\\\\right)\")\n",
    "            comp.scale(0.5)\n",
    "            comp.next_to(hidden_layer[0][i], RIGHT, buff=0.1)  # Increased buff\n",
    "            computations.add(comp)\n",
    "        self.play(Write(computations))\n",
    "        self.wait(1)\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in hidden_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide computations to avoid overlap with connections\n",
    "        self.play(FadeOut(computations))\n",
    "        \n",
    "\n",
    "        self.wait(0.5)\n",
    "\n",
    "        #self.animate_wave(self.connections_oh, color=YELLOW, direction='forward',run_time=1)\n",
    "        onnections_oh_to_hidden = self.connections_oh[:len(input_layer[1]) * len(hidden_layer[1])]\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[conn.animate.set_color(YELLOW) for conn in onnections_oh_to_hidden],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Show computations at output layer\n",
    "        computations_out = VGroup()\n",
    "        for i, neuron_label in enumerate(output_layer[1]):\n",
    "            # Display computation next to each output neuron\n",
    "            comp = MathTex(f\"=\\\\sigma\\\\left(\\\\sum_{{j=1}}^{{4}} v_{{{i+1}j}} h_j + c_{{{i+1}}}\\\\right)\")\n",
    "            comp.scale(0.5)\n",
    "            comp.next_to(output_layer[0][i], RIGHT, buff=0.1)  # Increased buff\n",
    "            computations_out.add(comp)\n",
    "        self.play(Write(computations_out))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide computations to avoid overlap\n",
    "        self.play(FadeOut(computations_out))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Activate output layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in output_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        # Show connections and weights again\n",
    "        self.play(FadeIn(connections), FadeIn(weights),run_time=1)\n",
    "        \n",
    "\n",
    "    def backpropagation(self, input_layer, hidden_layer, output_layer, connections, weights):\n",
    "        # Remove 'Forward Pass' text\n",
    "        self.play(FadeOut(self.forward_pass_text),FadeOut(self.Bias))\n",
    "\n",
    "        # Hide connections and weights during computations\n",
    "        self.play(FadeOut(connections), FadeOut(weights))\n",
    "\n",
    "        # Add 'Backpropagation' text\n",
    "        backprop_text = Text(\"Backpropagation\", font_size=30)\n",
    "        backprop_text.to_corner(DOWN + LEFT)\n",
    "        COST_FUNCTION = Text(\"C = Cost Function\", font_size=17)\n",
    "        COST_FUNCTION.to_corner(RIGHT).shift(RIGHT * 0.3)\n",
    "        self.add(backprop_text)\n",
    "        self.backprop_text = backprop_text\n",
    "        self.add(COST_FUNCTION)\n",
    "        self.COST_FUNCTION = COST_FUNCTION\n",
    "\n",
    "        # Indicate error at output layer\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in output_layer[0]],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at output layer\n",
    "        derivatives_out = VGroup()\n",
    "        for i, neuron_label in enumerate(output_layer[1]):\n",
    "            # Display derivative next to each output neuron\n",
    "            der = MathTex(f\"d_{{z_{{{i+1}}}}} =  \\\\frac{{\\\\partial C}}{{\\\\partial z_{{{i+1}}}}}\")\n",
    "            der.scale(0.5)\n",
    "            der.next_to(output_layer[0][i], LEFT, buff=0.5)  # Position derivatives on the left\n",
    "            derivatives_out.add(der)\n",
    "        self.play(Write(derivatives_out))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide derivative computations to avoid overlap\n",
    "        self.play(FadeOut(derivatives_out))\n",
    "        self.animate_wave(self.connections_oh, color=RED, direction= 'backward',run_time=1)\n",
    "       \n",
    "\n",
    "        # Activate hidden layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in hidden_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at hidden layer\n",
    "        derivatives_hidden = VGroup()\n",
    "        for i, neuron_label in enumerate(hidden_layer[1]):\n",
    "            # Display derivative next to each hidden neuron\n",
    "            der = MathTex(f\"\\\\nu_{{{i+1},t}} = \\\\sum_{{k=1}}^{{2}} \\\\frac{{\\\\partial C}}{{\\\\partial z_{{k}}}} \\\\frac{{\\\\partial z_{{k}}}}{{\\\\partial v_{{{i+1},t}}}}\")\n",
    "\n",
    "            der.scale(0.5)\n",
    "            der.next_to(hidden_layer[0][i], LEFT, buff=0.17)\n",
    "            derivatives_hidden.add(der)\n",
    "        self.play(Write(derivatives_hidden))\n",
    "        self.wait(1)\n",
    "\n",
    "        \n",
    "\n",
    "        # Hide derivative computations to avoid overlap\n",
    "        self.play(FadeOut(derivatives_hidden))\n",
    "        self.animate_wave(self.connections_ih, color=RED,direction='backward',run_time=1)\n",
    "        \n",
    "\n",
    "        # Activate input layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in input_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at input layer\n",
    "        derivatives_input = VGroup()\n",
    "        for i, neuron_label in enumerate(input_layer[1]):\n",
    "\n",
    "            der = MathTex(f\"\\\\omega_{{{i+1},j}} = \\\\sum_{{m=1}}^{{2}} \\\\sum_{{k=1}}^{{4}} \\\\frac{{\\\\partial C}}{{\\\\partial z_{{m}}}} \\\\cdot \\\\frac{{\\\\partial z_{{m}}}}{{\\\\partial h_{{k}}}} \\\\frac{{\\\\partial h_{{k}}}}{{\\\\partial w_{{{i+1},j }}}}\",font_size=45)\n",
    "\n",
    "            der.scale(0.5)\n",
    "            der.next_to(input_layer[0][i], LEFT, buff=0.11)\n",
    "            derivatives_input.add(der)\n",
    "        self.play(Write(derivatives_input))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide derivative computations\n",
    "        self.play(FadeOut(derivatives_input))\n",
    "        self.play(FadeOut(self.COST_FUNCTION),FadeOut(self.SIGMOID),FadeOut(backprop_text))\n",
    "        self.play(FadeOut(connections), FadeOut(weights),FadeOut(self.layers),FadeOut(self.layer_labels),FadeOut(self.SIGMOID))\n",
    "        Gradient = Text(\"After  Gradient  Descent On  The  Weights,  We  Update  The  Weights\", font_size=30)  \n",
    "        Example = MathTex(\"\\\\text{For Exapmle}  : \\\\tilde w_{i,j} = w_{i,j} - \\\\epsilon \\\\cdot \\\\omega_{i,j}\", font_size=35) \n",
    "        Repeat = MathTex(\"\\\\text{Repeat  This  Process  Until  The  Cost  Function  Is  Minimized}\", font_size=35)\n",
    "        Gradient.shift(UP * 1)\n",
    "        Repeat.to_edge(DOWN)\n",
    "\n",
    "        arrow = Arrow(LEFT,RIGHT,stroke_width=5,stroke_color=WHITE,fill_color=BLUE,fill_opacity=0.5, buff=0.1).shift(LEFT * 4.5)\n",
    "\n",
    "        self.play(LaggedStart(FadeIn(Gradient)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(Example)),run_time=1)\n",
    "        self.wait(2)\n",
    "        self.play(FadeOut(Gradient),FadeOut(Example))\n",
    "        self.play(FadeIn(connections.shift(RIGHT * 0.4)), FadeIn(self.weights_after.shift(RIGHT * 0.4)),\n",
    "                FadeIn(self.layers.shift(RIGHT * 0.4)),FadeIn(self.layer_labels.shift(RIGHT * 0.4)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(arrow)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(Repeat)),run_time=1)\n",
    "\n",
    "        self.wait(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMC visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -qm -v WARNING HMCLogisticRegressionAnimation26\n",
    "\n",
    "class HMCLogisticRegressionAnimation26(Scene):\n",
    "    def construct(self):\n",
    "        n_features = X.shape[1] \n",
    "        Intro_text = Text(\"MB\", font_size=24, weight= BOLD,color= BLUE_D).to_edge(UP).shift(LEFT*6.05)\n",
    "        self.add(Intro_text)\n",
    "        Neural_net_text = Text(\"HMC sampler\", font_size=18, weight= BOLD,color= BLUE_E).next_to(Intro_text,DOWN,buff=0.2).shift(RIGHT*0.1)\n",
    "        self.add(Neural_net_text)\n",
    "        \n",
    "        # Logistic regression functions\n",
    "        def sigmoid(z):\n",
    "            return np.where(z >= 0, \n",
    "                            1 / (1 + np.exp(-z)), \n",
    "                            np.exp(z) / (1 + np.exp(z)))\n",
    "        \n",
    "        # Define log-prior, log-likelihood, and their gradients\n",
    "        def log_prior(beta, tau=10):\n",
    "            return -0.5 * np.sum((beta / tau) ** 2) - len(beta) * np.log(tau * np.sqrt(2 * np.pi))\n",
    "        \n",
    "        def log_likelihood(beta, X, y):\n",
    "            linear_term = np.dot(X, beta)\n",
    "            return np.sum(y * linear_term - np.log1p(np.exp(linear_term)))\n",
    "        \n",
    "        def log_posterior(beta, X, y, tau=10):\n",
    "            return log_prior(beta, tau) + log_likelihood(beta, X, y)\n",
    "        \n",
    "        def gradient_log_posterior(beta, X, y, tau=10):\n",
    "            linear_term = np.dot(X, beta)\n",
    "            p = sigmoid(linear_term)\n",
    "            grad_likelihood = np.dot(X.T, (y - p))\n",
    "            grad_prior = -beta / tau**2\n",
    "            return grad_likelihood + grad_prior\n",
    "        \n",
    "        # Hamiltonian dynamics functions remain the same\n",
    "        def leapfrog_traj(beta, r, grad_log_posterior, step_size, leapfrog_steps, X, y, tau=10):\n",
    "            beta_new = np.copy(beta)\n",
    "            r_new = np.copy(r)\n",
    "            beta_traj = [np.copy(beta_new)]\n",
    "            r_new += 0.5 * step_size * grad_log_posterior(beta_new, X, y, tau)\n",
    "            for _ in range(leapfrog_steps):\n",
    "                beta_new += step_size * r_new\n",
    "                beta_traj.append(np.copy(beta_new))\n",
    "                if _ != leapfrog_steps - 1:\n",
    "                    r_new += step_size * grad_log_posterior(beta_new, X, y, tau)\n",
    "            r_new += 0.5 * step_size * grad_log_posterior(beta_new, X, y, tau)\n",
    "            return beta_new, r_new, beta_traj\n",
    "        \n",
    "        # HMC parameters\n",
    "        step_size = 0.045\n",
    "        leapfrog_steps = 40\n",
    "        num_iterations = 20\n",
    "        accept_count = 0\n",
    "        \n",
    "        # Initial position\n",
    "        beta_current = np.zeros(n_features)\n",
    "        beta_samples = []\n",
    "        beta_traj_samples = []\n",
    "        \n",
    "        # Labels\n",
    "        beta_label = MathTex(\"\\\\beta \\\\text{ (projected)}\",font_size =25).to_edge(UP).shift(LEFT*2.2,DOWN*0.4)\n",
    "        self.play(Write(beta_label))\n",
    "        \n",
    "        # Collect initial beta_samples to fit PCA\n",
    "        initial_samples = []\n",
    "        num_initial_samples = 300  # Increase for better estimation\n",
    "        temp_beta_current = np.copy(beta_current)\n",
    "        for i in range(num_initial_samples):\n",
    "            r = np.random.normal(0, 1, size=n_features)\n",
    "            beta_new, r_new, beta_traj = leapfrog_traj(temp_beta_current, r, gradient_log_posterior, step_size, leapfrog_steps, X, y)\n",
    "            current_H = -log_posterior(temp_beta_current, X, y) + 0.5 * np.dot(r, r)\n",
    "            new_H = -log_posterior(beta_new, X, y) + 0.5 * np.dot(r_new, r_new)\n",
    "            delta_H = new_H - current_H\n",
    "            if np.random.uniform() < np.exp(-delta_H):\n",
    "                temp_beta_current = beta_new\n",
    "            initial_samples.append(temp_beta_current)\n",
    "        \n",
    "        \n",
    "        # Fit PCA on initial_samples\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(initial_samples)\n",
    "        \n",
    "        # Get the ranges of the projected beta samples\n",
    "        projected_samples = pca.transform(initial_samples)\n",
    "        x_min, x_max = projected_samples[:, 0].min(), projected_samples[:, 0].max()\n",
    "        y_min, y_max = projected_samples[:, 1].min(), projected_samples[:, 1].max()\n",
    "\n",
    "        # Extend the ranges slightly for better visualization\n",
    "        x_margin = (x_max - x_min) * 0.1\n",
    "        y_margin = (y_max - y_min) * 0.1\n",
    "        x_min -= x_margin\n",
    "        x_max += x_margin\n",
    "        y_min -= y_margin\n",
    "        y_max += y_margin\n",
    "\n",
    "        # Create axes with these ranges\n",
    "        axes = Axes(\n",
    "            x_range=np.round([x_min, x_max, (x_max - x_min)/5], 2),\n",
    "            y_range=np.round([y_min, y_max, (y_max - y_min)/5], 2),\n",
    "            x_length=7.2,\n",
    "            y_length=7.2,\n",
    "            tips=False\n",
    "        ).add_coordinates()\n",
    "        axes.center()\n",
    "        self.play(Create(axes))\n",
    "        \n",
    "        # Create a grid over the PCA space\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 50),\n",
    "            np.linspace(y_min, y_max, 50)\n",
    "        )\n",
    "\n",
    "        # Flatten the grid points\n",
    "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "        # Reconstruct beta for each grid point\n",
    "        beta_grid = pca.inverse_transform(grid_points)\n",
    "\n",
    "        # Compute log_posterior for each beta in beta_grid\n",
    "        log_posterior_vals = np.array([log_posterior(beta, X, y) for beta in beta_grid])\n",
    "\n",
    "        # Reshape back to grid shape\n",
    "        LOG_POSTERIOR = log_posterior_vals.reshape(xx.shape)\n",
    "\n",
    "        # Create a matplotlib figure and axes with black background\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "        # Set the background color of the figure and axes to black\n",
    "        fig.patch.set_facecolor('black')\n",
    "        ax.set_facecolor('black')\n",
    "\n",
    "        # Plot the filled contour\n",
    "        max_log_posterior = np.max(LOG_POSTERIOR)\n",
    "        min_log_posterior = np.min(LOG_POSTERIOR)\n",
    "        contour_levels = np.linspace(min_log_posterior, max_log_posterior, 15)\n",
    "\n",
    "        # Use a colormap that stands out against a black background\n",
    "        cmap = plt.cm.plasma  # You can choose 'viridis', 'inferno', 'magma', etc.\n",
    "\n",
    "        # Plot the filled contour\n",
    "        CS = ax.contourf(xx, yy, LOG_POSTERIOR, levels=contour_levels, cmap=cmap)\n",
    "\n",
    "        # Optional: Add contour lines for reference, in a color that stands out\n",
    "        ax.contour(xx, yy, LOG_POSTERIOR, levels=contour_levels, colors='white', linewidths=0.5)\n",
    "\n",
    "        # Remove axis labels and ticks for a cleaner look\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Adjust colorbar with white labels and black background\n",
    "        #cbar = fig.colorbar(CS, ax=ax)\n",
    "        #cbar.ax.yaxis.set_tick_params(color='white')\n",
    "        #plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
    "        #cbar.outline.set_edgecolor('white')\n",
    "        #cbar.ax.set_facecolor('black')\n",
    "\n",
    "        # Convert the plot to an image that Manim can display\n",
    "        fig.canvas.draw()\n",
    "        contour_image = ImageMobject(np.array(fig.canvas.renderer.buffer_rgba()))\n",
    "        contour_image.scale_to_fit_width(axes.width)\n",
    "        contour_image.move_to(axes)\n",
    "\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "        # Display the filled contour in Manim\n",
    "        self.add(contour_image)\n",
    "        self.bring_to_back(contour_image)\n",
    "\n",
    "\n",
    "        \n",
    "        # Initialize the current_dot\n",
    "        current_proj = pca.transform([beta_current])[0]\n",
    "        current_point = axes.coords_to_point(current_proj[0], current_proj[1])\n",
    "        current_dot = Dot(current_point, color=BLUE)\n",
    "        self.play(FadeIn(current_dot))\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            r = np.random.normal(0, 1, size=n_features)\n",
    "            start_beta = np.copy(beta_current)\n",
    "            beta_new, r_new, beta_traj = leapfrog_traj(beta_current, r, gradient_log_posterior, step_size, leapfrog_steps, X, y)\n",
    "            \n",
    "            # Hamiltonian at current and proposed positions\n",
    "            current_H = -log_posterior(beta_current, X, y) + 0.5 * np.dot(r, r)\n",
    "            new_H = -log_posterior(beta_new, X, y) + 0.5 * np.dot(r_new, r_new)\n",
    "            delta_H = new_H - current_H\n",
    "            \n",
    "            # Metropolis acceptance criterion\n",
    "            if np.random.uniform() < np.exp(-delta_H):\n",
    "                beta_current = beta_new\n",
    "                accept_count += 1\n",
    "                accepted = True\n",
    "            else:\n",
    "                beta_current = start_beta\n",
    "                accepted = False\n",
    "            \n",
    "            beta_samples.append(beta_current)\n",
    "            beta_traj_samples.extend(beta_traj)\n",
    "            \n",
    "            # Project the trajectory using fixed PCA components\n",
    "            projected_traj = pca.transform(beta_traj)\n",
    "            \n",
    "            # Convert projected points to Manim coordinates\n",
    "            traj_points = [axes.coords_to_point(pt[0], pt[1]) for pt in projected_traj]\n",
    "            traj = VMobject()\n",
    "            traj.set_points_smoothly(traj_points)\n",
    "            traj.set_stroke(color=PURPLE)\n",
    "            \n",
    "            # Animate the dot moving along the trajectory\n",
    "            self.play(MoveAlongPath(current_dot, traj), Create(traj), run_time=0.9)\n",
    "            self.play(Transform(traj, traj.copy().set_stroke(color=PURPLE, width=0.15)))\n",
    "            \n",
    "            # Indicate acceptance or rejection\n",
    "            if accepted:\n",
    "                self.play(Indicate(current_dot, color=GREEN, scale_factor=1.2))\n",
    "            else:\n",
    "                self.play(Indicate(current_dot, color=RED, scale_factor=1.2))\n",
    "                # Move the dot back to the start position\n",
    "                start_proj = pca.transform([start_beta])[0]\n",
    "                start_point = axes.coords_to_point(start_proj[0], start_proj[1])\n",
    "                self.play(current_dot.animate.move_to(start_point))\n",
    "        \n",
    "        # Display acceptance rate\n",
    "        acceptance_rate = accept_count / num_iterations\n",
    "        acceptance_text = Text(f\"Acceptance Rate: {acceptance_rate:.2f}\", font_size=24).to_edge(RIGHT).shift(RIGHT*0.38)\n",
    "        self.play(Write(acceptance_text))\n",
    "        self.wait(2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
