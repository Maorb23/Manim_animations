{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -qm -v WARNING NeuralNetworkSceneLastMB\n",
    "\n",
    "\n",
    "class NeuralNetworkSceneLastMB(Scene):\n",
    "\n",
    "    def construct(self):\n",
    "        # Create layers without layer labels\n",
    "        input_layer = self.create_layer(3, \"x\")\n",
    "        hidden_layer = self.create_layer(4, \"h\")\n",
    "        output_layer = self.create_layer(2, \"z\")\n",
    "        Intro_text = Text(\"MB\", font_size=24, weight= BOLD,color= BLUE_D).to_edge(UP).shift(LEFT*6.05)\n",
    "        self.add(Intro_text)\n",
    "        Neural_net_text = Text(\"Neural Network\", font_size=16, weight= BOLD,color= BLUE_E).next_to(Intro_text,DOWN,buff=0.2) #.shift(RIGHT*0.1)\n",
    "        self.add(Neural_net_text)\n",
    "\n",
    "        # Position layers\n",
    "        layers = VGroup(input_layer, hidden_layer, output_layer)\n",
    "        layers.arrange(RIGHT, buff=2.7)\n",
    "        self.add(layers)\n",
    "        self.layers = layers\n",
    "\n",
    "        # Create layer labels\n",
    "        input_label = Text(\"Input Layer\", font_size=24)\n",
    "        hidden_label = Text(\"Hidden Layer\", font_size=24)\n",
    "        output_label = Text(\"Output Layer\", font_size=24)\n",
    "        layer_labels = VGroup(input_label, hidden_label, output_label)\n",
    "\n",
    "        # Position layer labels above the layers at the same height\n",
    "        for label, layer in zip(layer_labels, layers):\n",
    "            label.move_to(layer.get_center())\n",
    "            label.shift(UP * 3.2)  \n",
    "\n",
    "        self.add(layer_labels)\n",
    "        self.layer_labels = layer_labels\n",
    "\n",
    "        # Draw connections with weights\n",
    "        connections_ih, weights_ih,weights_ih_after = self.connect_layers(input_layer, hidden_layer, \"w\")\n",
    "        connections_oh, weights_ho,weights_oh_after = self.connect_layers(hidden_layer, output_layer, \"v\")\n",
    "        self.connections_oh = connections_oh\n",
    "        self.connections_ih = connections_ih\n",
    "\n",
    "        # Combine all connections and weights\n",
    "        connections = connections_ih + connections_oh\n",
    "        weights = weights_ih + weights_ho\n",
    "        weights_after = weights_ih_after + weights_oh_after\n",
    "        self.weights_after = weights_after\n",
    "\n",
    "        # Add connections and weight labels to the scene\n",
    "        self.add(connections, weights)\n",
    "\n",
    "        # Animate forward pass with computations\n",
    "        self.forward_pass(input_layer, hidden_layer, output_layer, connections, weights)\n",
    "\n",
    "        # Animate backpropagation with derivatives\n",
    "        self.backpropagation(input_layer, hidden_layer, output_layer, connections, weights)\n",
    "\n",
    "    def animate_wave(self, connections, color, direction='forward', run_time=1):\n",
    " \n",
    "        dots = VGroup()\n",
    "        #lines = VGroup() #optional\n",
    "        animations = []\n",
    "        \n",
    "        for connection in connections:\n",
    "            # Reverse the connection path if direction is 'backward'\n",
    "            if direction == 'backward':\n",
    "                path = Line(\n",
    "                    connection.get_end(),\n",
    "                    connection.get_start(),\n",
    "                    stroke_color=connection.get_stroke_color(),\n",
    "                    stroke_width=connection.get_stroke_width()\n",
    "                )\n",
    "            else:\n",
    "                path = connection.copy()\n",
    "            \n",
    "            # Create a dot at the start of the path\n",
    "            dot = Dot(color=color, radius=0.05)\n",
    "            dot.move_to(path.get_start())\n",
    "            dots.add(dot)\n",
    "            \n",
    "            # Animate the dot moving along the path\n",
    "            animation = MoveAlongPath(dot, path, run_time=run_time, rate_func=linear)\n",
    "            animations.append(animation)\n",
    "        \n",
    "        \n",
    "        #self.add(lines) #optional\n",
    "        #self.remove(lines) #optional\n",
    "        self.add(dots)\n",
    "        self.play(*animations, lag_ratio=0)\n",
    "        self.remove(dots)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def create_layer(self, num_neurons, label_prefix):\n",
    "        \"\"\"\n",
    "        Creates a layer with a specified number of neurons and labels each neuron uniquely.\n",
    "        \"\"\"\n",
    "        neurons = VGroup()\n",
    "        neuron_labels = VGroup()\n",
    "        for i in range(num_neurons):\n",
    "            neuron = Circle(radius=0.3, color=BLUE, fill_opacity=0)\n",
    "            neurons.add(neuron)\n",
    "        neurons.arrange(DOWN, buff=1)  # Increased buff for more vertical spacing\n",
    "        for i, neuron in enumerate(neurons):\n",
    "            # Create unique labels like x₁, x₂, etc.\n",
    "            label = MathTex(f\"{label_prefix}_{{{i+1}}}\")\n",
    "            label.scale(0.6)\n",
    "            label.next_to(neuron, UP, buff=0.05)  # if we want to change the label to be inside the circle, change LEFT to UP\n",
    "            neuron_labels.add(label)\n",
    "        layer = VGroup(neurons, neuron_labels)\n",
    "        return layer  # Return layer without layer label\n",
    "\n",
    "    def connect_layers(self, layer1, layer2, weight_prefix):\n",
    "\n",
    "        connections = VGroup()\n",
    "        weight_labels = VGroup()\n",
    "        weight_labels_after = VGroup()\n",
    "        neurons1 = layer1[0]  \n",
    "        neurons2 = layer2[0]\n",
    "        for i, neuron1 in enumerate(neurons1):\n",
    "            for j, neuron2 in enumerate(neurons2):\n",
    "                connection = Line(\n",
    "                    neuron1.get_right(),\n",
    "                    neuron2.get_left(),\n",
    "                    stroke_color=GREY,\n",
    "                    stroke_width=2\n",
    "                )\n",
    "                connections.add(connection)\n",
    "                weight_label = MathTex(f\"{weight_prefix}_{{{j+1}{i+1}}}\")\n",
    "                weight_label_new = MathTex(f\"\\\\tilde {weight_prefix}_{{{j+1}{i+1}}}\",font_size=16)\n",
    "                weight_label.scale(0.5)\n",
    "                midpoint = connection.get_midpoint()\n",
    "                direction = neuron2.get_left() - neuron1.get_right()\n",
    "                unit_direction = direction / np.linalg.norm(direction)\n",
    "                perp_direction = np.array([-unit_direction[1], unit_direction[0], 0])\n",
    "                weight_label.move_to(midpoint + perp_direction * 0.5)\n",
    "                weight_labels.add(weight_label)\n",
    "                weight_label_new.move_to(midpoint + perp_direction * 0.5)\n",
    "                weight_labels_after.add(weight_label_new)\n",
    "\n",
    "        return connections, weight_labels,weight_labels_after\n",
    "\n",
    "    def forward_pass(self, input_layer, hidden_layer, output_layer, connections, weights):\n",
    "\n",
    "        forward_pass = Text(\"Forward Pass\", font_size=30)\n",
    "        forward_pass.to_corner(DOWN + LEFT)\n",
    "        SIGMOID = MathTex(\"\\\\sigma(x) = \\\\frac{1}{1 + e^{-x}}\",font_size=20)\n",
    "        Bias = MathTex(\"b_j, c_t, \\\\quad j \\\\in [1,4], t \\\\in [1,2]: \\quad \\\\text{Bias  Terms}\",font_size=20)\n",
    "        SIGMOID.to_corner(UP + RIGHT).shift(DOWN * 0.6)  \n",
    "        Bias.next_to(SIGMOID,DOWN).shift(DOWN * 0.05,LEFT * 0.7)\n",
    "        self.add(forward_pass)\n",
    "        self.forward_pass_text = forward_pass\n",
    "        self.add(SIGMOID)\n",
    "        self.add(Bias)\n",
    "        self.SIGMOID = SIGMOID\n",
    "        self.Bias = Bias\n",
    "        self.wait(2)\n",
    "\n",
    "        \n",
    "        self.play(FadeOut(connections), FadeOut(weights)) # Hide connections and weights during computations\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in input_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        connections_ih_to_hidden = self.connections_ih[:len(input_layer[1]) * len(hidden_layer[1])]\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[conn.animate.set_color(YELLOW) for conn in connections_ih_to_hidden],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show computations at hidden layer\n",
    "        computations = VGroup()\n",
    "        for i, neuron_label in enumerate(hidden_layer[1]):\n",
    "            # Display computation next to each hidden neuron\n",
    "            comp = MathTex(f\"=\\\\sigma\\\\left(\\\\sum_{{j=1}}^{{3}} w_{{{i+1}j}} x_j + b_{{{i+1}}}\\\\right)\")\n",
    "            comp.scale(0.5)\n",
    "            comp.next_to(hidden_layer[0][i], RIGHT, buff=0.1)  # Increased buff\n",
    "            computations.add(comp)\n",
    "        self.play(Write(computations))\n",
    "        self.wait(1)\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in hidden_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide computations to avoid overlap with connections\n",
    "        self.play(FadeOut(computations))\n",
    "        \n",
    "\n",
    "        self.wait(0.5)\n",
    "\n",
    "        #self.animate_wave(self.connections_oh, color=YELLOW, direction='forward',run_time=1)\n",
    "        onnections_oh_to_hidden = self.connections_oh[:len(input_layer[1]) * len(hidden_layer[1])]\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[conn.animate.set_color(YELLOW) for conn in onnections_oh_to_hidden],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Show computations at output layer\n",
    "        computations_out = VGroup()\n",
    "        for i, neuron_label in enumerate(output_layer[1]):\n",
    "            # Display computation next to each output neuron\n",
    "            comp = MathTex(f\"=\\\\sigma\\\\left(\\\\sum_{{j=1}}^{{4}} v_{{{i+1}j}} h_j + c_{{{i+1}}}\\\\right)\")\n",
    "            comp.scale(0.5)\n",
    "            comp.next_to(output_layer[0][i], RIGHT, buff=0.1)  # Increased buff\n",
    "            computations_out.add(comp)\n",
    "        self.play(Write(computations_out))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide computations to avoid overlap\n",
    "        self.play(FadeOut(computations_out))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Activate output layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[neuron.animate.set_fill(YELLOW, opacity=0.5) for neuron in output_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "\n",
    "        # Show connections and weights again\n",
    "        self.play(FadeIn(connections), FadeIn(weights),run_time=1)\n",
    "        \n",
    "\n",
    "    def backpropagation(self, input_layer, hidden_layer, output_layer, connections, weights):\n",
    "        # Remove 'Forward Pass' text\n",
    "        self.play(FadeOut(self.forward_pass_text),FadeOut(self.Bias))\n",
    "\n",
    "        # Hide connections and weights during computations\n",
    "        self.play(FadeOut(connections), FadeOut(weights))\n",
    "\n",
    "        # Add 'Backpropagation' text\n",
    "        backprop_text = Text(\"Backpropagation\", font_size=30)\n",
    "        backprop_text.to_corner(DOWN + LEFT)\n",
    "        COST_FUNCTION = Text(\"C = Cost Function\", font_size=17)\n",
    "        COST_FUNCTION.to_corner(RIGHT).shift(RIGHT * 0.3)\n",
    "        self.add(backprop_text)\n",
    "        self.backprop_text = backprop_text\n",
    "        self.add(COST_FUNCTION)\n",
    "        self.COST_FUNCTION = COST_FUNCTION\n",
    "\n",
    "        # Indicate error at output layer\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in output_layer[0]],\n",
    "                lag_ratio=0.01\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at output layer\n",
    "        derivatives_out = VGroup()\n",
    "        for i, neuron_label in enumerate(output_layer[1]):\n",
    "            # Display derivative next to each output neuron\n",
    "            der = MathTex(f\"d_{{z_{{{i+1}}}}} =  \\\\frac{{\\\\partial C}}{{\\\\partial z_{{{i+1}}}}}\")\n",
    "            der.scale(0.5)\n",
    "            der.next_to(output_layer[0][i], LEFT, buff=0.5)  # Position derivatives on the left\n",
    "            derivatives_out.add(der)\n",
    "        self.play(Write(derivatives_out))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide derivative computations to avoid overlap\n",
    "        self.play(FadeOut(derivatives_out))\n",
    "        self.animate_wave(self.connections_oh, color=RED, direction= 'backward',run_time=1)\n",
    "       \n",
    "\n",
    "        # Activate hidden layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in hidden_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at hidden layer\n",
    "        derivatives_hidden = VGroup()\n",
    "        for i, neuron_label in enumerate(hidden_layer[1]):\n",
    "            # Display derivative next to each hidden neuron\n",
    "            der = MathTex(f\"\\\\nu_{{{i+1},t}} = \\\\sum_{{k=1}}^{{2}} \\\\frac{{\\\\partial C}}{{\\\\partial z_{{k}}}} \\\\frac{{\\\\partial z_{{k}}}}{{\\\\partial v_{{{i+1},t}}}}\")\n",
    "\n",
    "            der.scale(0.5)\n",
    "            der.next_to(hidden_layer[0][i], LEFT, buff=0.17)\n",
    "            derivatives_hidden.add(der)\n",
    "        self.play(Write(derivatives_hidden))\n",
    "        self.wait(1)\n",
    "\n",
    "        \n",
    "\n",
    "        # Hide derivative computations to avoid overlap\n",
    "        self.play(FadeOut(derivatives_hidden))\n",
    "        self.animate_wave(self.connections_ih, color=RED,direction='backward',run_time=1)\n",
    "        \n",
    "\n",
    "        # Activate input layer neurons\n",
    "        self.play(\n",
    "            LaggedStart(\n",
    "                *[Indicate(neuron, color=RED) for neuron in input_layer[0]],\n",
    "                lag_ratio=0.1\n",
    "            )\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "\n",
    "        # Show derivative computations at input layer\n",
    "        derivatives_input = VGroup()\n",
    "        for i, neuron_label in enumerate(input_layer[1]):\n",
    "\n",
    "            der = MathTex(f\"\\\\omega_{{{i+1},j}} = \\\\sum_{{m=1}}^{{2}} \\\\sum_{{k=1}}^{{4}} \\\\frac{{\\\\partial C}}{{\\\\partial z_{{m}}}} \\\\cdot \\\\frac{{\\\\partial z_{{m}}}}{{\\\\partial h_{{k}}}} \\\\frac{{\\\\partial h_{{k}}}}{{\\\\partial w_{{{i+1},j }}}}\",font_size=45)\n",
    "\n",
    "            der.scale(0.5)\n",
    "            der.next_to(input_layer[0][i], LEFT, buff=0.11)\n",
    "            derivatives_input.add(der)\n",
    "        self.play(Write(derivatives_input))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Hide derivative computations\n",
    "        self.play(FadeOut(derivatives_input))\n",
    "        self.play(FadeOut(self.COST_FUNCTION),FadeOut(self.SIGMOID),FadeOut(backprop_text))\n",
    "        self.play(FadeOut(connections), FadeOut(weights),FadeOut(self.layers),FadeOut(self.layer_labels),FadeOut(self.SIGMOID))\n",
    "        Gradient = Text(\"After  Gradient  Descent On  The  Weights,  We  Update  The  Weights\", font_size=30)  \n",
    "        Example = MathTex(\"\\\\text{For Exapmle}  : \\\\tilde w_{i,j} = w_{i,j} - \\\\epsilon \\\\cdot \\\\omega_{i,j}\", font_size=35) \n",
    "        Repeat = MathTex(\"\\\\text{Repeat  This  Process  Until  The  Cost  Function  Is  Minimized}\", font_size=35)\n",
    "        Gradient.shift(UP * 1)\n",
    "        Repeat.to_edge(DOWN)\n",
    "\n",
    "        arrow = Arrow(LEFT,RIGHT,stroke_width=5,stroke_color=WHITE,fill_color=BLUE,fill_opacity=0.5, buff=0.1).shift(LEFT * 4.5)\n",
    "\n",
    "        self.play(LaggedStart(FadeIn(Gradient)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(Example)),run_time=1)\n",
    "        self.wait(2)\n",
    "        self.play(FadeOut(Gradient),FadeOut(Example))\n",
    "        self.play(FadeIn(connections.shift(RIGHT * 0.4)), FadeIn(self.weights_after.shift(RIGHT * 0.4)),\n",
    "                FadeIn(self.layers.shift(RIGHT * 0.4)),FadeIn(self.layer_labels.shift(RIGHT * 0.4)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(arrow)),run_time=1)\n",
    "        self.play(LaggedStart(FadeIn(Repeat)),run_time=1)\n",
    "\n",
    "        self.wait(2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
